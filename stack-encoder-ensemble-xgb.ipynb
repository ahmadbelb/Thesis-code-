{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import model\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-10-01 00:00:00</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-10-01 00:00:30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-10-01 00:01:00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-10-01 00:01:30</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-10-01 00:02:00</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  volume\n",
       "0  2016-10-01 00:00:00      11\n",
       "1  2016-10-01 00:00:30       5\n",
       "2  2016-10-01 00:01:00       6\n",
       "3  2016-10-01 00:01:30      13\n",
       "4  2016-10-01 00:02:00       9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/ahmadbelbeisi/Desktop/bitcoin_ticker.csv')\n",
    "date_ori = pd.to_datetime(df.iloc[:, 0]).tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahmadbelbeisi/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "minmax = MinMaxScaler().fit(df.iloc[:, 1].values.reshape((-1,1)))\n",
    "close_normalize = minmax.transform(df.iloc[:, 1].values.reshape((-1,1))).reshape((-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(977584,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_normalize.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder:\n",
    "    def __init__(self, input_, dimension = 2, learning_rate = 0.01, hidden_layer = 256, epoch = 20):\n",
    "        input_size = input_.shape[1]\n",
    "        self.X = tf.placeholder(\"float\", [None, input_.shape[1]])\n",
    "    \n",
    "        weights = {\n",
    "        'encoder_h1': tf.Variable(tf.random_normal([input_size, hidden_layer])),\n",
    "        'encoder_h2': tf.Variable(tf.random_normal([hidden_layer, dimension])),\n",
    "        'decoder_h1': tf.Variable(tf.random_normal([dimension, hidden_layer])),\n",
    "        'decoder_h2': tf.Variable(tf.random_normal([hidden_layer, input_size])),\n",
    "        }\n",
    "    \n",
    "        biases = {\n",
    "        'encoder_b1': tf.Variable(tf.random_normal([hidden_layer])),\n",
    "        'encoder_b2': tf.Variable(tf.random_normal([dimension])),\n",
    "        'decoder_b1': tf.Variable(tf.random_normal([hidden_layer])),\n",
    "        'decoder_b2': tf.Variable(tf.random_normal([input_size])),\n",
    "        }\n",
    "    \n",
    "        first_layer_encoder = tf.nn.sigmoid(tf.add(tf.matmul(self.X, weights['encoder_h1']), biases['encoder_b1']))\n",
    "        self.second_layer_encoder = tf.nn.sigmoid(tf.add(tf.matmul(first_layer_encoder, weights['encoder_h2']), biases['encoder_b2']))\n",
    "        first_layer_decoder = tf.nn.sigmoid(tf.add(tf.matmul(self.second_layer_encoder, weights['decoder_h1']), biases['decoder_b1']))\n",
    "        second_layer_decoder = tf.nn.sigmoid(tf.add(tf.matmul(first_layer_decoder, weights['decoder_h2']), biases['decoder_b2']))\n",
    "        self.cost = tf.reduce_mean(tf.pow(self.X - second_layer_decoder, 2))\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(self.cost)\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            last_time = time.time()\n",
    "            _, loss = self.sess.run([self.optimizer, self.cost], feed_dict={self.X: input_})\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print('epoch:', i + 1, 'loss:', loss, 'time:', time.time() - last_time)\n",
    "    \n",
    "    def encode(self, input_):\n",
    "        return self.sess.run(self.second_layer_encoder, feed_dict={self.X: input_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 loss: 0.13798071 time: 2.227764844894409\n",
      "epoch: 20 loss: 0.13798054 time: 1.969930648803711\n",
      "epoch: 30 loss: 0.1379799 time: 2.001722812652588\n",
      "epoch: 40 loss: 0.13797113 time: 2.0925800800323486\n",
      "epoch: 50 loss: 0.13796951 time: 2.0357439517974854\n",
      "epoch: 60 loss: 0.13796605 time: 2.246385097503662\n",
      "epoch: 70 loss: 0.13795693 time: 2.153808116912842\n",
      "epoch: 80 loss: 0.13794802 time: 2.1015753746032715\n",
      "epoch: 90 loss: 0.13787717 time: 2.279877185821533\n",
      "epoch: 100 loss: 0.13766761 time: 2.0313339233398438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(977584, 32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "Encoder=encoder(close_normalize.reshape((-1,1)), 32, 0.01, 128, 100)\n",
    "thought_vector = Encoder.encode(close_normalize.reshape((-1,1)))\n",
    "thought_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import *\n",
    "ada = AdaBoostRegressor(n_estimators=500, learning_rate=0.1)\n",
    "bagging = BaggingRegressor(n_estimators=500)\n",
    "et = ExtraTreesRegressor(n_estimators=500)\n",
    "gb = GradientBoostingRegressor(n_estimators=500, learning_rate=0.1)\n",
    "rf = RandomForestRegressor(n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada.fit(thought_vector[:-1, :], close_normalize[1:])\n",
    "bagging.fit(thought_vector[:-1, :], close_normalize[1:])\n",
    "et.fit(thought_vector[:-1, :], close_normalize[1:])\n",
    "gb.fit(thought_vector[:-1, :], close_normalize[1:])\n",
    "rf.fit(thought_vector[:-1, :], close_normalize[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(32), ada.feature_importances_)\n",
    "plt.title('ada boost important feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(32), et.feature_importances_)\n",
    "plt.title('et important feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(32), gb.feature_importances_)\n",
    "plt.title('gb important feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(32), rf.feature_importances_)\n",
    "plt.title('rf important feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_pred=ada.predict(thought_vector)\n",
    "bagging_pred=bagging.predict(thought_vector)\n",
    "et_pred=et.predict(thought_vector)\n",
    "gb_pred=gb.predict(thought_vector)\n",
    "rf_pred=rf.predict(thought_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_actual = np.hstack([close_normalize[0],ada_pred[:-1]])\n",
    "bagging_actual = np.hstack([close_normalize[0],bagging_pred[:-1]])\n",
    "et_actual = np.hstack([close_normalize[0],et_pred[:-1]])\n",
    "gb_actual = np.hstack([close_normalize[0],gb_pred[:-1]])\n",
    "rf_actual = np.hstack([close_normalize[0],rf_pred[:-1]])\n",
    "stack_predict = np.vstack([ada_actual,bagging_actual,et_actual,gb_actual,rf_actual,close_normalize]).T\n",
    "corr_df = pd.DataFrame(stack_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr_df.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wow, I do not expect this heatmap. Totally a heat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "params_xgd = {\n",
    "    'max_depth': 7,\n",
    "    'objective': 'reg:logistic',\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 10000\n",
    "    }\n",
    "train_Y = close_normalize[1:]\n",
    "clf = xgb.XGBRegressor(**params_xgd)\n",
    "clf.fit(stack_predict[:-1,:],train_Y, eval_set=[(stack_predict[:-1,:],train_Y)], \n",
    "        eval_metric='rmse', early_stopping_rounds=20, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = clf.predict(stack_predict)\n",
    "xgb_actual = np.hstack([close_normalize[0],xgb_pred[:-1]])\n",
    "date_original=pd.Series(date_ori).dt.strftime(date_format='%Y-%m-%d').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_close(array):\n",
    "    return minmax.inverse_transform(array.reshape((-1,1))).reshape((-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,6))\n",
    "x_range = np.arange(df.Close.shape[0])\n",
    "plt.plot(x_range, df.Close, label = 'Real Close')\n",
    "plt.plot(x_range, reverse_close(ada_pred), label = 'ada Close')\n",
    "plt.plot(x_range, reverse_close(bagging_pred), label = 'bagging Close')\n",
    "plt.plot(x_range, reverse_close(et_pred), label = 'et Close')\n",
    "plt.plot(x_range, reverse_close(gb_pred), label = 'gb Close')\n",
    "plt.plot(x_range, reverse_close(rf_pred), label = 'rf Close')\n",
    "plt.plot(x_range, reverse_close(xgb_pred), label = 'xgb stacked Close')\n",
    "plt.legend()\n",
    "plt.xticks(x_range[::50], date_original[::50])\n",
    "plt.title('stacked')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_list = ada_pred.tolist()\n",
    "bagging_list = bagging_pred.tolist()\n",
    "et_list = et_pred.tolist()\n",
    "gb_list = gb_pred.tolist()\n",
    "rf_list = rf_pred.tolist()\n",
    "xgb_list = xgb_pred.tolist()\n",
    "def predict(count, history = 5):\n",
    "    for i in range(count):\n",
    "        roll = np.array(xgb_list[-history:])\n",
    "        thought_vector = Encoder.encode(roll.reshape((-1,1)))\n",
    "        ada_pred=ada.predict(thought_vector)\n",
    "        bagging_pred=bagging.predict(thought_vector)\n",
    "        et_pred=et.predict(thought_vector)\n",
    "        gb_pred=gb.predict(thought_vector)\n",
    "        rf_pred=rf.predict(thought_vector)\n",
    "        ada_list.append(ada_pred[-1])\n",
    "        bagging_list.append(bagging_pred[-1])\n",
    "        et_list.append(et_pred[-1])\n",
    "        gb_list.append(gb_pred[-1])\n",
    "        rf_list.append(rf_pred[-1])\n",
    "        ada_actual = np.hstack([xgb_list[-history],ada_pred[:-1]])\n",
    "        bagging_actual = np.hstack([xgb_list[-history],bagging_pred[:-1]])\n",
    "        et_actual = np.hstack([xgb_list[-history],et_pred[:-1]])\n",
    "        gb_actual = np.hstack([xgb_list[-history],gb_pred[:-1]])\n",
    "        rf_actual = np.hstack([xgb_list[-history],rf_pred[:-1]])\n",
    "        stack_predict = np.vstack([ada_actual,bagging_actual,et_actual,gb_actual,rf_actual,xgb_list[-history:]]).T\n",
    "        xgb_pred = clf.predict(stack_predict)\n",
    "        xgb_list.append(xgb_pred[-1])\n",
    "        date_ori.append(date_ori[-1]+timedelta(days=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(30, history = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,6))\n",
    "x_range = np.arange(df.Close.shape[0])\n",
    "x_range_future = np.arange(len(xgb_list))\n",
    "plt.plot(x_range, df.Close, label = 'Real Close')\n",
    "plt.plot(x_range_future, reverse_close(np.array(ada_list)), label = 'ada Close')\n",
    "plt.plot(x_range_future, reverse_close(np.array(bagging_list)), label = 'bagging Close')\n",
    "plt.plot(x_range_future, reverse_close(np.array(et_list)), label = 'et Close')\n",
    "plt.plot(x_range_future, reverse_close(np.array(gb_list)), label = 'gb Close')\n",
    "plt.plot(x_range_future, reverse_close(np.array(rf_list)), label = 'rf Close')\n",
    "plt.plot(x_range_future, reverse_close(np.array(xgb_list)), label = 'xgb stacked Close')\n",
    "plt.legend()\n",
    "plt.xticks(x_range_future[::50], pd.Series(date_ori).dt.strftime(date_format='%Y-%m-%d').tolist()[::50])\n",
    "plt.title('stacked')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
